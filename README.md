# Benchmarking LLMs' capabilities to generate structured outputs

This repository contains a benchmark of OpenAI's GPT-4o and GPT-4o-mini models to generate structured outputs comparing the performance of the models in terms of cost, latency, and error rate.

### Usage

> You must have [Bun](https://bun.sh/) installed to run this benchmark.
> You must have an OpenAI API key to use this benchmark (`OPENAI_API_KEY`).

```bash
bun i
bun run ./index.ts
```
